import pandas as pd
import json
import os
import torch
from setfit import SetFitModel, Trainer, TrainingArguments
from datasets import Dataset
from sentence_transformers.losses import CosineSimilarityLoss
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# --- Cáº¤U HÃŒNH ---
TRAIN_FILE = "train.csv"
TEST_FILE = "test.csv" 
MODEL_PATH = "my-smart-home-model"

def load_and_prepare_data(train_path, test_path):
    if not os.path.exists(train_path) or not os.path.exists(test_path):
        raise FileNotFoundError("KhÃ´ng tÃ¬m tháº¥y file dataset.")
    
    df_train = pd.read_csv(train_path)
    df_test = pd.read_csv(test_path)
    
    # 1. Lá»c dá»¯ liá»‡u rÃ¡c
    df_train = df_train[df_train['label_str'] != 'label_str']
    df_test = df_test[df_test['label_str'] != 'label_str']
    df_train = df_train.dropna(subset=['label_str', 'text'])
    df_test = df_test.dropna(subset=['label_str', 'text'])

    # 2. Táº¡o Map tá»« táº­p TRAIN (ÄÃ¢y lÃ  bá»™ chuáº©n)
    unique_labels = sorted(df_train['label_str'].unique().tolist())
    label2id = {label: i for i, label in enumerate(unique_labels)}
    id2label = {i: label for i, label in enumerate(unique_labels)}
    
    print(f"-> ÄÃ£ load {len(df_train)} dÃ²ng train, {len(df_test)} dÃ²ng test.")
    print("-> Danh sÃ¡ch nhÃ£n:", label2id)

    # 3. Map dá»¯ liá»‡u
    df_train['label'] = df_train['label_str'].map(label2id)
    
    # LÆ°u Ã½: Náº¿u test cÃ³ nhÃ£n láº¡ chÆ°a tá»«ng train, ta sáº½ bá» qua Ä‘á»ƒ trÃ¡nh lá»—i
    df_test['label'] = df_test['label_str'].map(label2id)
    df_test = df_test.dropna(subset=['label']) # Bá» dÃ²ng nhÃ£n láº¡
    
    return df_train, df_test, label2id, id2label

# --- MAIN ---
print("=== 1. ÄANG Táº¢I Dá»® LIá»†U ===")
df_train, df_test, label2id, id2label = load_and_prepare_data(TRAIN_FILE, TEST_FILE)

train_dataset = Dataset.from_pandas(df_train)
test_dataset = Dataset.from_pandas(df_test)

print("\n=== 2. ÄANG Táº¢I MODEL ===")
# Check GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ”¥ DEVICE: {device.upper()}")

model = SetFitModel.from_pretrained(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    device=device
)

# TÄƒng epochs lÃªn 3 Ä‘á»ƒ model há»c ká»¹ hÆ¡n cÃ¡c nhÃ£n con
args = TrainingArguments(
    batch_size=16,
    num_epochs=2, # <--- TÄ‚NG LÃŠN 3
    loss=CosineSimilarityLoss,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    column_mapping={"text": "text", "label": "label"}
)

print("\n=== 3. Báº®T Äáº¦U TRAIN ===")
trainer.train()

print("\n=== 4. ÄÃNH GIÃ ===")
preds = model(df_test['text'].tolist())
y_pred = preds.tolist()
y_true = df_test['label'].tolist()

acc = accuracy_score(y_true, y_pred)
print(f"Äá»˜ CHÃNH XÃC: {acc * 100:.2f}%")

print("\n--- BÃ¡o cÃ¡o chi tiáº¿t ---")
# --- FIX Lá»–I REPORT TRIá»†T Äá»‚ ---
# Láº¥y danh sÃ¡ch ID cá»§a TOÃ€N Bá»˜ nhÃ£n (tá»« táº­p train)
all_labels_ids = list(id2label.keys())
all_target_names = list(id2label.values())

# Ã‰p hÃ m report in ra Ä‘á»§ danh sÃ¡ch nÃ y, náº¿u táº­p test thiáº¿u thÃ¬ nÃ³ ghi 0
print(classification_report(
    y_true, 
    y_pred, 
    labels=all_labels_ids, 
    target_names=all_target_names, 
    zero_division=0
))

print("\n--- CÃ¡c cÃ¢u sai ---")
for i in range(len(y_true)):
    if y_true[i] != y_pred[i]:
        print(f"CÃ¢u: '{df_test.iloc[i]['text']}'")
        print(f"   Thá»±c táº¿: {id2label[y_true[i]]} | MÃ¡y Ä‘oÃ¡n: {id2label[y_pred[i]]}")

# LÆ°u model
model.save_pretrained(MODEL_PATH)
with open(f"{MODEL_PATH}/label_map.json", "w", encoding="utf-8") as f:
    json.dump(id2label, f, ensure_ascii=False, indent=4)
print("\nÄÃ£ lÆ°u model xong.")